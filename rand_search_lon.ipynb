{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a0d379a",
   "metadata": {},
   "source": [
    "# DATA PROCESSING FOR XGB TRAINED ON EVERY DAY DISTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5e0442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "import numpy as np\n",
    "import geopandas as gpd \n",
    "from calendar import monthrange\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import uniform, randint\n",
    "import gc\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04a11164",
   "metadata": {},
   "outputs": [],
   "source": [
    "ports = pd.read_csv('ports.csv', sep ='|')\n",
    "# Clean port data\n",
    "ports = ports.drop(columns=['name', 'portLocation', 'UN_LOCODE', 'countryName', 'ISO'], errors='ignore')\n",
    "# Rename latitude and longitude to distinguish them\n",
    "ports = ports.rename(columns={\n",
    "    'latitude': 'port_latitude',\n",
    "    'longitude': 'port_longitude'\n",
    "})\n",
    "\n",
    "def preprocess(df):\n",
    "    # Replacing default with Nan bacause too close to valid values, eliminate non valid values\n",
    "    df['cog'] = df['cog'].replace(360, np.nan)\n",
    "    df = df[(df['cog'] <= 360) | (df['cog'].isna())]\n",
    "\n",
    "    # Replacing default with Nan bacause too close to valid values\n",
    "    df['sog'] = df['sog'].replace(1023, np.nan)\n",
    "\n",
    "    # Replacing default with Nan bacause too close to valid values\n",
    "    # Changing uncertain values to bigger number to be further away from sample pool\n",
    "    # Adding uncertainty flag\n",
    "    df['rot'] = df['rot'].replace(128, np.nan)\n",
    "    df['rot'] = df['rot'].replace({127: 200, -127: -200})\n",
    "    df['uncertain_rot'] = np.where(df['rot'].isin([200, -200]), 1, 0)\n",
    "\n",
    "    # Replacing default value with NaN to not get taken in consideration by regression\n",
    "    df['heading'] = df['heading'].replace(511, np.nan)\n",
    "\n",
    "    # Adding a \"is moored?\" flag\n",
    "    df['isMoored'] = (df['navstat'] == 5).astype(int)   \n",
    "\n",
    "    # Time Handling \n",
    "    df['time'] = pd.to_datetime(df['time'], errors='coerce').dt.tz_localize('UTC')\n",
    "    # Standardize eta\n",
    "    df['etaRaw'] = df['etaRaw'].fillna(0)\n",
    "    df['etaRaw'] = df['etaRaw'].apply(lambda x: f\"{2024}-{x}\")\n",
    "    df['etaRaw'] = pd.to_datetime(df['etaRaw'], errors='coerce').dt.tz_localize('UTC')\n",
    "    df.rename(columns={'etaRaw': 'etaStd'}, inplace=True)\n",
    "    # Handle first month of the years ETA year to be 2023\n",
    "    df['etaStd'] = df.apply(lambda row: row['etaStd'].replace(year=row['etaStd'].year - 1)\n",
    "                            if row['etaStd'].month in [11, 12] and row['time'].month in [1, 2] \n",
    "                            else row['etaStd'], axis=1) \n",
    "    \n",
    "\n",
    "    # FEATURE ENGINEERING\n",
    "    # Get day of the week \n",
    "    df['dayofweek'] = df['time'].dt.dayofweek\n",
    "    df['eta_dayoftheweek'] = df['etaStd'].dt.dayofweek \n",
    "    # Converts time and eta to seconds and add difference between the two\n",
    "    df['time_seq'] = df['time'].astype(int) / 10**9  \n",
    "    df['eta_seq'] = df['etaStd'].astype(int) / 10**9 \n",
    "    df['estimated_time_left'] = df['time_seq'] - df['eta_seq']\n",
    "    # Add port coordinates\n",
    "    df = pd.merge(df, ports, on='portId', how='left')\n",
    "    # Add a three days rolling average for the AIS data \n",
    "    df['sog_mean'] = df.groupby('vesselId', group_keys=False).apply(\n",
    "    lambda x: x.sort_values('time').rolling('3D', on='time')['sog'].mean())\n",
    "    df['cog_mean'] = df.groupby('vesselId', group_keys=False).apply(\n",
    "    lambda x: x.sort_values('time').rolling('3D', on='time')['cog'].mean())\n",
    "    df['rot_mean'] = df.groupby('vesselId', group_keys=False).apply(\n",
    "    lambda x: x.sort_values('time').rolling('3D', on='time')['rot'].mean())\n",
    "    df['heading_mean'] = df.groupby('vesselId', group_keys=False).apply(\n",
    "    lambda x: x.sort_values('time').rolling('3D', on='time')['heading'].mean())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b5c45e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_set(df, steps):\n",
    "    df_copy = df.copy()\n",
    "    df_copy.sort_values(by=['vesselId', 'time'], inplace=True)\n",
    "    \n",
    "    # FEATURE ENGINEERING\n",
    "    # Vessels last collenction data\n",
    "    df_copy['latitude_lag'] = df_copy.groupby('vesselId')['latitude'].shift(steps)\n",
    "    df_copy['longitude_lag'] = df_copy.groupby('vesselId')['longitude'].shift(steps)\n",
    "    df_copy['port_longitude_lag'] = df_copy.groupby('vesselId')['port_longitude'].shift(steps)\n",
    "    df_copy['port_latitude_lag'] = df_copy.groupby('vesselId')['port_latitude'].shift(steps)\n",
    "    df_copy['isMoored_lag'] = df_copy.groupby('vesselId')['isMoored'].shift(steps)\n",
    "    df_copy['sog_lag'] = df_copy.groupby('vesselId')['sog'].shift(steps)\n",
    "    df_copy['sog_mean_lag'] = df_copy.groupby('vesselId')['sog_mean'].shift(steps)\n",
    "    df_copy['cog_lag'] = df_copy.groupby('vesselId')['cog'].shift(steps)\n",
    "    df_copy['cog_mean_lag'] = df_copy.groupby('vesselId')['cog_mean'].shift(steps)     \n",
    "    df_copy['rot_lag'] = df_copy.groupby('vesselId')['rot'].shift(steps) \n",
    "    df_copy['rot_mean_lag'] = df_copy.groupby('vesselId')['rot_mean'].shift(steps)\n",
    "    df_copy['uncertain_rot_lag'] = df_copy.groupby('vesselId')['uncertain_rot'].shift(steps) \n",
    "    df_copy['heading_lag'] = df_copy.groupby('vesselId')['heading'].shift(steps)\n",
    "    df_copy['heading_mean_lag'] = df_copy.groupby('vesselId')['heading_mean'].shift(steps)  \n",
    "    df_copy['dayofweek_lag'] = df_copy.groupby('vesselId')['dayofweek'].shift(steps)\n",
    "    # Time since last data collection\n",
    "    df_copy['time_diff'] = df_copy.groupby('vesselId')['time'].diff(steps)\n",
    "    df_copy['time_diff_seconds'] = df_copy['time_diff'].dt.total_seconds()\n",
    "    # Time to eta\n",
    "    df_copy['estimated_time_left_lag'] = df_copy.groupby('vesselId')['estimated_time_left'].shift(steps)\n",
    "    df_copy.dropna(subset=['time_diff'], inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e305698d",
   "metadata": {},
   "source": [
    "# TRAIN ON DIFFERENT TIME HORIZONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5033b9",
   "metadata": {},
   "source": [
    "## Process train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce5d4a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_56071/44196829.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['sog_mean'] = df.groupby('vesselId', group_keys=False).apply(\n",
      "/var/tmp/ipykernel_56071/44196829.py:57: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['cog_mean'] = df.groupby('vesselId', group_keys=False).apply(\n",
      "/var/tmp/ipykernel_56071/44196829.py:59: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['rot_mean'] = df.groupby('vesselId', group_keys=False).apply(\n",
      "/var/tmp/ipykernel_56071/44196829.py:61: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['heading_mean'] = df.groupby('vesselId', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "# Load train data\n",
    "known_positions = pd.read_csv('ais_train.csv', sep ='|')  # Replace with your dataset\n",
    "test = pd.read_csv('ais_test.csv', sep =',')\n",
    "# Preprocess train\n",
    "known_positions = preprocess(known_positions)\n",
    "train = known_positions.copy()\n",
    "# Create training sets with variable time differences\n",
    "train1 = make_training_set(train, 1)\n",
    "train2 = make_training_set(train, 2)\n",
    "train3 = make_training_set(train, 3)\n",
    "train4 = make_training_set(train, 4)\n",
    "train5 = make_training_set(train, 5)\n",
    "train6 = make_training_set(train, 6)\n",
    "train7 = make_training_set(train, 7)\n",
    "train8 = make_training_set(train, 8)\n",
    "train9 = make_training_set(train, 9)\n",
    "train10 = make_training_set(train, 10)\n",
    "train11 = make_training_set(train, 11)\n",
    "train12 = make_training_set(train, 12)\n",
    "train13 = make_training_set(train, 13)\n",
    "train14 = make_training_set(train, 14)\n",
    "train15 = make_training_set(train, 15)\n",
    "train16 = make_training_set(train, 16)\n",
    "train17 = make_training_set(train, 17)\n",
    "train18 = make_training_set(train, 18)\n",
    "train19 = make_training_set(train, 19) # 24 hours\n",
    "train20 = make_training_set(train, 20) \n",
    "train22 = make_training_set(train, 22) \n",
    "train24 = make_training_set(train, 24) \n",
    "train26 = make_training_set(train, 26) \n",
    "train28 = make_training_set(train, 28) \n",
    "train30 = make_training_set(train, 30) \n",
    "train32 = make_training_set(train, 32) \n",
    "train34 = make_training_set(train, 34)\n",
    "train36 = make_training_set(train, 36)\n",
    "train38 = make_training_set(train, 38) # 48 hours\n",
    "train40 = make_training_set(train, 40) \n",
    "train44 = make_training_set(train, 44) \n",
    "train48 = make_training_set(train, 48) \n",
    "train52 = make_training_set(train, 52) \n",
    "train56 = make_training_set(train, 56) \n",
    "train60 = make_training_set(train, 60) # 74 hours\n",
    "train64 = make_training_set(train, 64) \n",
    "train68 = make_training_set(train, 68) \n",
    "train72 = make_training_set(train, 72)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eae48a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([\n",
    "    train1, train2, train3, train4, train5, train6, train7, train8, train9, train10,\n",
    "    train11, train12, train13, train14, train15, train16, train17, train18, train19, train20,\n",
    "    train22, train24, train26, train28, train30, train32, train34, train36, train38, train40,\n",
    "    train44, train48, train52, train56, train60, train64, train68, train72,\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ef1c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['navstat'] = pd.Categorical(train['navstat']).codes\n",
    "train['portId'] = pd.Categorical(train['portId']).codes\n",
    "# Encoding test and train vesselID with the same encoder \n",
    "unique_vessel_ids = pd.concat([known_positions['vesselId'], test['vesselId']]).unique()\n",
    "vessel_encoder = LabelEncoder()\n",
    "vessel_encoder.fit(unique_vessel_ids)\n",
    "# Transform the vesselId column in train\n",
    "train['vesselId'] = vessel_encoder.transform(train['vesselId'])\n",
    "\n",
    "# Clean missing data\n",
    "train = train.dropna(subset=['latitude', 'longitude', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36b87688",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = train[[\n",
    "    'vesselId', #try\n",
    "    'latitude_lag',\n",
    "    'longitude_lag',\n",
    "    'port_latitude_lag',\n",
    "    'port_longitude_lag',\n",
    "    'isMoored_lag',\n",
    "    'sog_lag',\n",
    "    'sog_mean_lag',\n",
    "    'cog_lag',\n",
    "    'cog_mean_lag',\n",
    "    'rot_lag',\n",
    "    'rot_mean_lag',\n",
    "    'uncertain_rot_lag',\n",
    "    'heading_lag',\n",
    "    'heading_mean_lag',\n",
    "    'dayofweek_lag',\n",
    "    'time_diff_seconds',\n",
    "    'estimated_time_left_lag',\n",
    "]]\n",
    "y = train['longitude']\n",
    "\n",
    "# Creating test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce') # Not needed\n",
    "\n",
    "# Sampling train to do make hyper opt faster and less memeoy intensive\n",
    "train_sampled = train.sample(frac=0.1, random_state=42)\n",
    "\n",
    "X_train_sampled = train_sampled[[\n",
    "    'vesselId', #try\n",
    "    'latitude_lag',\n",
    "    'longitude_lag',\n",
    "    'port_latitude_lag',\n",
    "    'port_longitude_lag',\n",
    "    'isMoored_lag',\n",
    "    'sog_lag',\n",
    "    'sog_mean_lag',\n",
    "    'cog_lag',\n",
    "    'cog_mean_lag',\n",
    "    'rot_lag',\n",
    "    'rot_mean_lag',\n",
    "    'uncertain_rot_lag',\n",
    "    'heading_lag',\n",
    "    'heading_mean_lag',\n",
    "    'dayofweek_lag',\n",
    "    'time_diff_seconds',\n",
    "    'estimated_time_left_lag',\n",
    "]]\n",
    "y_train_sampled = train_sampled[['longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbfabcfa-c00e-4847-a5b5-8b1c4c9653b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(868872432), np.int64(91460256))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sampled.memory_usage(deep=True).sum(), y_train_sampled.memory_usage(deep=True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d4c24f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best parameters: {'learning_rate': np.float64(0.2787310710802003), 'max_depth': 9, 'min_child_weight': 4, 'n_estimators': 289, 'subsample': np.float64(0.8650660661526529)}\n"
     ]
    }
   ],
   "source": [
    "# Standard scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_sampled)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Model setup\n",
    "model_lon = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "# Parameter distribution for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(100, 300),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'learning_rate': uniform(0.01, 0.29),\n",
    "    'min_child_weight': randint(1, 6),\n",
    "    'subsample': uniform(0.8, 0.2)\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV without early stopping and using cross-validation on full X_train\n",
    "random_search = RandomizedSearchCV(\n",
    "    model_lon,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=4,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "# Fit model (note: only passing X_train_scaled and y_train)\n",
    "random_search.fit(X_train_scaled, y_train_sampled)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "\n",
    "# Best estimator from the search\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_pred_val = best_model.predict(X_val_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "536bbabf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8107568413107765)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = mean_absolute_error(y_val, y_pred_val)\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4ea9e4e-2d5e-4775-a180-fbabf3cb1323",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13241505     22.95291\n",
       "8601587    -125.30561\n",
       "6401453      28.34455\n",
       "23021813      0.41560\n",
       "1637764     153.16738\n",
       "Name: longitude, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.head()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
