{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a166c5a",
   "metadata": {},
   "source": [
    "[120]Magna Graecia\\\n",
    "Sergio Enrico Pisoni 132855\\\n",
    "Sofia Papaioannou 132898\\\n",
    "Lefteris Verouchis 132873"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0d379a",
   "metadata": {},
   "source": [
    "# DATA PROCESSING FOR XGB TRAINED ON EVERY DAY DISTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5e0442d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_importance\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/__init__.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/compat/__init__.py:29\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressors\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m     is_numpy_dev,\n\u001b[1;32m     27\u001b[0m     np_version_under1p21,\n\u001b[1;32m     28\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     pa_version_under7p0,\n\u001b[1;32m     31\u001b[0m     pa_version_under8p0,\n\u001b[1;32m     32\u001b[0m     pa_version_under9p0,\n\u001b[1;32m     33\u001b[0m     pa_version_under11p0,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_function_name\u001b[39m(f: F, name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m F:\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    Bind the name/qualname attributes of the function.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/compat/pyarrow.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     _pa_version \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39m__version__\n\u001b[1;32m     11\u001b[0m     _palv \u001b[38;5;241m=\u001b[39m Version(_pa_version)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyarrow/__init__.py:65\u001b[0m\n\u001b[1;32m     63\u001b[0m _gc_enabled \u001b[38;5;241m=\u001b[39m _gc\u001b[38;5;241m.\u001b[39misenabled()\n\u001b[1;32m     64\u001b[0m _gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_lib\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _gc_enabled:\n\u001b[1;32m     67\u001b[0m     _gc\u001b[38;5;241m.\u001b[39menable()\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:405\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "import numpy as np\n",
    "import geopandas as gpd \n",
    "from calendar import monthrange\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from shapely.ops import nearest_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a11164",
   "metadata": {},
   "outputs": [],
   "source": [
    "ports = pd.read_csv('ports.csv', sep ='|')\n",
    "# Clean port data\n",
    "ports = ports.drop(columns=['name', 'portLocation', 'UN_LOCODE', 'countryName', 'ISO'], errors='ignore')\n",
    "# Rename latitude and longitude to distinguish them\n",
    "ports = ports.rename(columns={\n",
    "    'latitude': 'port_latitude',\n",
    "    'longitude': 'port_longitude'\n",
    "})\n",
    "\n",
    "def preprocess(df):\n",
    "    # Replacing default with Nan bacause too close to valid values, eliminate non valid values\n",
    "    df['cog'] = df['cog'].replace(360, np.nan)\n",
    "    df = df[(df['cog'] <= 360) | (df['cog'].isna())]\n",
    "\n",
    "    # Replacing default with Nan bacause too close to valid values\n",
    "    df['sog'] = df['sog'].replace(1023, np.nan)\n",
    "\n",
    "    # Replacing default with Nan bacause too close to valid values\n",
    "    # Changing uncertain values to bigger number to be further away from sample pool\n",
    "    # Adding uncertainty flag\n",
    "    df['rot'] = df['rot'].replace(128, np.nan)\n",
    "    df['rot'] = df['rot'].replace({127: 200, -127: -200})\n",
    "    df['uncertain_rot'] = np.where(df['rot'].isin([200, -200]), 1, 0)\n",
    "\n",
    "    # Replacing default value with NaN to not get taken in consideration by regression\n",
    "    df['heading'] = df['heading'].replace(511, np.nan)\n",
    "\n",
    "    # Adding a \"is moored?\" flag\n",
    "    df['isMoored'] = (df['navstat'] == 5).astype(int)   \n",
    "\n",
    "    # Time Handling \n",
    "    df['time'] = pd.to_datetime(df['time'], errors='coerce').dt.tz_localize('UTC')\n",
    "    # Standardize eta\n",
    "    df['etaRaw'] = df['etaRaw'].fillna(0)\n",
    "    df['etaRaw'] = df['etaRaw'].apply(lambda x: f\"{2024}-{x}\")\n",
    "    df['etaRaw'] = pd.to_datetime(df['etaRaw'], errors='coerce').dt.tz_localize('UTC')\n",
    "    df.rename(columns={'etaRaw': 'etaStd'}, inplace=True)\n",
    "    # Handle first month of the years ETA year to be 2023\n",
    "    df['etaStd'] = df.apply(lambda row: row['etaStd'].replace(year=row['etaStd'].year - 1)\n",
    "                            if row['etaStd'].month in [11, 12] and row['time'].month in [1, 2] \n",
    "                            else row['etaStd'], axis=1) \n",
    "    \n",
    "\n",
    "    # FEATURE ENGINEERING\n",
    "    # Get day of the week \n",
    "    df['dayofweek'] = df['time'].dt.dayofweek\n",
    "    df['eta_dayoftheweek'] = df['etaStd'].dt.dayofweek \n",
    "    # Converts time and eta to seconds and add difference between the two\n",
    "    df['time_seq'] = df['time'].astype(int) / 10**9  \n",
    "    df['eta_seq'] = df['etaStd'].astype(int) / 10**9 \n",
    "    df['estimated_time_left'] = df['time_seq'] - df['eta_seq']\n",
    "    # Add port coordinates\n",
    "    df = pd.merge(df, ports, on='portId', how='left')\n",
    "    # Add a three days rolling average for the AIS data \n",
    "    df['sog_mean'] = df.groupby('vesselId', group_keys=False).apply(\n",
    "    lambda x: x.sort_values('time').rolling('3D', on='time')['sog'].mean())\n",
    "    df['cog_mean'] = df.groupby('vesselId', group_keys=False).apply(\n",
    "    lambda x: x.sort_values('time').rolling('3D', on='time')['cog'].mean())\n",
    "    df['rot_mean'] = df.groupby('vesselId', group_keys=False).apply(\n",
    "    lambda x: x.sort_values('time').rolling('3D', on='time')['rot'].mean())\n",
    "    df['heading_mean'] = df.groupby('vesselId', group_keys=False).apply(\n",
    "    lambda x: x.sort_values('time').rolling('3D', on='time')['heading'].mean())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5c45e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_set(df, steps):\n",
    "    df_copy = df.copy()\n",
    "    df_copy.sort_values(by=['vesselId', 'time'], inplace=True)\n",
    "    \n",
    "    # FEATURE ENGINEERING\n",
    "    # Vessels last collenction data\n",
    "    df_copy['latitude_lag'] = df_copy.groupby('vesselId')['latitude'].shift(steps)\n",
    "    df_copy['longitude_lag'] = df_copy.groupby('vesselId')['longitude'].shift(steps)\n",
    "    df_copy['port_longitude_lag'] = df_copy.groupby('vesselId')['port_longitude'].shift(steps)\n",
    "    df_copy['port_latitude_lag'] = df_copy.groupby('vesselId')['port_latitude'].shift(steps)\n",
    "    df_copy['isMoored_lag'] = df_copy.groupby('vesselId')['isMoored'].shift(steps)\n",
    "    df_copy['sog_lag'] = df_copy.groupby('vesselId')['sog'].shift(steps)\n",
    "    df_copy['sog_mean_lag'] = df_copy.groupby('vesselId')['sog_mean'].shift(steps)\n",
    "    df_copy['cog_lag'] = df_copy.groupby('vesselId')['cog'].shift(steps)\n",
    "    df_copy['cog_mean_lag'] = df_copy.groupby('vesselId')['cog_mean'].shift(steps)     \n",
    "    df_copy['rot_lag'] = df_copy.groupby('vesselId')['rot'].shift(steps) \n",
    "    df_copy['rot_mean_lag'] = df_copy.groupby('vesselId')['rot_mean'].shift(steps)\n",
    "    df_copy['uncertain_rot_lag'] = df_copy.groupby('vesselId')['uncertain_rot'].shift(steps) \n",
    "    df_copy['heading_lag'] = df_copy.groupby('vesselId')['heading'].shift(steps)\n",
    "    df_copy['heading_mean_lag'] = df_copy.groupby('vesselId')['heading_mean'].shift(steps)  \n",
    "    df_copy['dayofweek_lag'] = df_copy.groupby('vesselId')['dayofweek'].shift(steps)\n",
    "    # Time since last data collection\n",
    "    df_copy['time_diff'] = df_copy.groupby('vesselId')['time'].diff(steps)\n",
    "    df_copy['time_diff_seconds'] = df_copy['time_diff'].dt.total_seconds()\n",
    "    # Time to eta\n",
    "    df_copy['estimated_time_left_lag'] = df_copy.groupby('vesselId')['estimated_time_left'].shift(steps)\n",
    "    df_copy.dropna(subset=['time_diff'], inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e305698d",
   "metadata": {},
   "source": [
    "# TRAIN ON DIFFERENT TIME HORIZONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5033b9",
   "metadata": {},
   "source": [
    "## Process train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5d4a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "known_positions = pd.read_csv('ais_train.csv', sep ='|')  # Replace with your dataset\n",
    "test = pd.read_csv('ais_test.csv', sep =',')\n",
    "# Preprocess train\n",
    "known_positions = preprocess(known_positions)\n",
    "train = known_positions.copy()\n",
    "# Create training sets with variable time difference\n",
    "time_intervals = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 44, 48, 52, 56, 60, 64, 68, 72] #until 88hrs intervals\n",
    "train_sets = {}\n",
    "for interval in time_intervals:\n",
    "    train_sets[f'train{interval}'] = make_training_set(train, interval)\n",
    "\n",
    "train = pd.concat(list(train_sets.values()), ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef1c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['navstat'] = pd.Categorical(train['navstat']).codes\n",
    "train['portId'] = pd.Categorical(train['portId']).codes\n",
    "# Encoding test and train vesselID with the same encoder \n",
    "unique_vessel_ids = pd.concat([known_positions['vesselId'], test['vesselId']]).unique()\n",
    "vessel_encoder = LabelEncoder()\n",
    "vessel_encoder.fit(unique_vessel_ids)\n",
    "# Transform the vesselId column in train\n",
    "train['vesselId'] = vessel_encoder.transform(train['vesselId'])\n",
    "\n",
    "# Clean missing data\n",
    "train = train.dropna(subset=['latitude', 'longitude', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b87688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and evalutaion set\n",
    "X = train[[\n",
    "    'vesselId', #try\n",
    "    'latitude_lag',\n",
    "    'longitude_lag',\n",
    "    'port_latitude_lag',\n",
    "    'port_longitude_lag',\n",
    "    'isMoored_lag',\n",
    "    'sog_lag',\n",
    "    'sog_mean_lag',\n",
    "    'cog_lag',\n",
    "    'cog_mean_lag',\n",
    "    'rot_lag',\n",
    "    'rot_mean_lag',\n",
    "    'uncertain_rot_lag',\n",
    "    'heading_lag',\n",
    "    'heading_mean_lag',\n",
    "    'dayofweek_lag',\n",
    "    'time_diff_seconds',\n",
    "    'estimated_time_left_lag',\n",
    "]]\n",
    "y = train[['latitude', 'longitude']]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4c24f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7671690258773383"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = xgb.XGBRegressor(\n",
    "    learning_rate=0.24196263261990883,\n",
    "    max_depth=9,\n",
    "    min_child_weight=1,\n",
    "    n_estimators=185,\n",
    "    subsample=0.9136916108674354,\n",
    "    random_state=42\n",
    ")\n",
    "model = MultiOutputRegressor(base_model)\n",
    "# Fit\n",
    "model.fit(X_train, y_train)\n",
    "# Predict\n",
    "y_pred_val = model.predict(X_val)\n",
    "# Evaluate model\n",
    "mae = mean_absolute_error(y_val, y_pred_val)\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9d9183",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['time'] = pd.to_datetime(test['time'], errors='coerce').dt.tz_localize('UTC')\n",
    "test['vesselId'] = vessel_encoder.transform(test['vesselId'])\n",
    "known_positions['vesselId'] = vessel_encoder.transform(known_positions['vesselId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48202bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prediction_data(test_df, known_positions):\n",
    "    # Get last known position for each vessel before the test time\n",
    "    predictions_data = []\n",
    "    \n",
    "    # Group by vessel ID to avoid repeated processing\n",
    "    vessel_histories = dict(tuple(known_positions.groupby('vesselId')))\n",
    "    \n",
    "    for _, row in test_df.iterrows():\n",
    "        vessel_history = vessel_histories[row['vesselId']]\n",
    "        # Get last position before test time\n",
    "        last_detection = vessel_history[vessel_history['time'] < row['time']].iloc[-1]\n",
    "        \n",
    "        predictions_data.append({\n",
    "            'vesselId': last_detection['vesselId'],\n",
    "            'latitude_lag': last_detection['latitude'],\n",
    "            'longitude_lag': last_detection['longitude'],\n",
    "            'port_latitude_lag': last_detection['port_latitude'],\n",
    "            'port_longitude_lag': last_detection['port_longitude'],\n",
    "            'isMoored_lag': last_detection['isMoored'],\n",
    "            'sog_lag': last_detection['sog'],\n",
    "            'sog_mean_lag': last_detection['sog_mean'],\n",
    "            'cog_lag': last_detection['cog'],\n",
    "            'cog_mean_lag': last_detection['cog_mean'],\n",
    "            'rot_lag': last_detection['rot'],\n",
    "            'rot_mean_lag': last_detection['rot_mean'],\n",
    "            'uncertain_rot_lag': last_detection['uncertain_rot'],\n",
    "            'heading_lag': last_detection['heading'],\n",
    "            'heading_mean_lag': last_detection['heading_mean'],\n",
    "            'dayofweek_lag': last_detection['dayofweek'],\n",
    "            'time_diff_seconds': (pd.to_datetime(row['time']) - last_detection['time']).total_seconds(),\n",
    "            'estimated_time_left_lag': last_detection['estimated_time_left'],\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(predictions_data)\n",
    "\n",
    "# Prepare prediction data\n",
    "input_df = prepare_prediction_data(test, known_positions)\n",
    "# Make predictions for all rows at once\n",
    "predictions = model.predict(input_df)\n",
    "# Create results DataFrame\n",
    "results = pd.DataFrame({\n",
    "    'ID': test.index,\n",
    "    'longitude_predicted': predictions[:, 1],\n",
    "    'latitude_predicted': predictions[:, 0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf00aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('no_postproc_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d857fc",
   "metadata": {},
   "source": [
    "## Post processing\n",
    "Move all predictions to sea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6c76d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('ais_test.csv', sep=',') # reset test df\n",
    "test = test.drop(columns=['scaling_factor']) \n",
    "merged = pd.merge(results, test, on='ID', how='left')\n",
    "merged['time'] = pd.to_datetime(merged['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f58e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ocean and land shape files\n",
    "land_world = gpd.read_file('ne_10m_land/ne_10m_land.shp').to_crs(4326)\n",
    "ocean_world = gpd.read_file('ne_10m_ocean/ne_10m_ocean.shp').to_crs(4326)\n",
    "# Make Geo Data Frame\n",
    "gdf = gpd.GeoDataFrame(merged, geometry=gpd.points_from_xy(merged['longitude_predicted'], merged['latitude_predicted'], crs=\"EPSG:4326\"))\n",
    "# Check if point is on land\n",
    "points_on_land = gpd.sjoin(gdf, land_world, how=\"inner\", predicate=\"within\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33342afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get closest points in sea\n",
    "closest_longitudes = []\n",
    "closest_latitudes = []\n",
    "for _, row in points_on_land.iterrows():\n",
    "    closest_point, _ = nearest_points(ocean_world['geometry'], row['geometry'])\n",
    "    closest_longitudes.append(closest_point.x)\n",
    "    closest_latitudes.append(closest_point.y)\n",
    "points_on_land['closest_longitude'] = closest_longitudes\n",
    "points_on_land['closest_latitude'] = closest_latitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35af2201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/_j2m440s1cdcpmdx7hff9q0r0000gn/T/ipykernel_24704/3106212199.py:2: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  gdf.loc[points_on_land.index, 'longitude_predicted'] = points_on_land['closest_longitude'].astype(float)\n",
      "/var/folders/_k/_j2m440s1cdcpmdx7hff9q0r0000gn/T/ipykernel_24704/3106212199.py:3: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  gdf.loc[points_on_land.index, 'latitude_predicted'] = points_on_land['closest_latitude'].astype(float)\n"
     ]
    }
   ],
   "source": [
    "# Update predictions\n",
    "gdf.loc[points_on_land.index, 'longitude_predicted'] = points_on_land['closest_longitude'].astype(float)\n",
    "gdf.loc[points_on_land.index, 'latitude_predicted'] = points_on_land['closest_latitude'].astype(float)\n",
    "gdf = gdf.drop(columns=['vesselId', 'time', 'geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_csv('result_n1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
