import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error
from tqdm import tqdm

# Preprocessing function
def preprocess(df):
    # Convert 'time' to datetime and sort
    df['time'] = pd.to_datetime(df['time'])
    df.sort_values(by=['vesselId', 'time'], inplace=True)

    # Filter out moored vessels
    df['isMoored'] = df['navstat'] == 5
    df = df[~df['isMoored']]

    # Extract additional time-based features
    df['hour'] = df['time'].dt.hour
    df['day_of_week'] = df['time'].dt.dayofweek
    df['month'] = df['time'].dt.month
    df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)
    
    # Convert 'time' to numerical format for modeling
    df['time_seq'] = df['time'].astype('int64') / 10**9

    # Add rolling mean of speed over a 3-day window
    df['sog_mean'] = df.groupby('vesselId', group_keys=False).apply(
        lambda x: x.sort_values('time').rolling('3D', on='time')['sog'].mean()
    )
    df['sog_std'] = df.groupby('vesselId', group_keys=False).apply(
        lambda x: x.sort_values('time').rolling('3D', on='time')['sog'].std()
    )

    # Drop rows with NaN values
    df.dropna(inplace=True)
    return df

# Function to create lag features
def make_training_set(df, n_shifts):
    df['latitude_lag'] = df.groupby('vesselId')['latitude'].shift(n_shifts)
    df['longitude_lag'] = df.groupby('vesselId')['longitude'].shift(n_shifts)
    df['sog_lag'] = df.groupby('vesselId')['sog'].shift(n_shifts)
    df['cog_lag'] = df.groupby('vesselId')['cog'].shift(n_shifts) / 180 - 1  # Normalize
    df['rot_lag'] = df.groupby('vesselId')['rot'].shift(n_shifts)
    df['heading_lag'] = df.groupby('vesselId')['heading'].shift(n_shifts) / 180 - 1  # Normalize
    df['time_diff_seconds'] = df['time'].diff(n_shifts).dt.total_seconds()

    # Drop rows with NaN values after feature creation
    df.dropna(inplace=True)
    return df

# Load and preprocess the training data
train_data = pd.read_csv('ais_train.csv', sep='|')
train_data = preprocess(train_data)

# Create training sets with different lags
train1 = make_training_set(train_data, 1)
train2 = make_training_set(train_data, 2)
train3 = make_training_set(train_data, 3)

# Combine training sets for modeling
train_combined = pd.concat([train1, train2, train3], ignore_index=True)

# Define features and target variables
features = [
    'latitude_lag', 'longitude_lag', 'sog_lag', 'cog_lag', 'rot_lag', 'heading_lag', 
    'time_diff_seconds', 'sog_mean', 'sog_std', 'hour', 'day_of_week', 'is_weekend'
]
X = train_combined[features]
y_lat = train_combined['latitude']
y_lon = train_combined['longitude']

# Split into training and validation sets
X_train_lat, X_val_lat, y_train_lat, y_val_lat = train_test_split(X, y_lat, test_size=0.1, random_state=42)
X_train_lon, X_val_lon, y_train_lon, y_val_lon = train_test_split(X, y_lon, test_size=0.1, random_state=42)

# Hyperparameter tuning using GridSearchCV for latitude
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt']
}
rf = RandomForestRegressor(random_state=42)
grid_search_lat = GridSearchCV(rf, param_grid, cv=3, n_jobs=-1, scoring='neg_mean_absolute_error', verbose=2)
grid_search_lat.fit(X_train_lat, y_train_lat)

# Use the best estimator for latitude
best_rf_lat = grid_search_lat.best_estimator_
print("Best parameters for latitude model:", grid_search_lat.best_params_)

# Train model for longitude (using similar tuning or a pre-defined set)
rf = RandomForestRegressor(n_estimators=200, max_depth=30, random_state=42)
rf.fit(X_train_lon, y_train_lon)

# Make predictions on the validation set
y_pred_lat = best_rf_lat.predict(X_val_lat)
y_pred_lon = rf.predict(X_val_lon)

# Calculate mean absolute error
mae_lat = mean_absolute_error(y_val_lat, y_pred_lat)
mae_lon = mean_absolute_error(y_val_lon, y_pred_lon)

print(f'Mean Absolute Error for Latitude: {mae_lat}')
print(f'Mean Absolute Error for Longitude: {mae_lon}')

# Function to predict future positions
def predict_future_position(vessel_id, time, training_data, model_lat, model_lon):
    latest_data_points = training_data[training_data['vesselId'] == vessel_id].sort_values(by='time')
    latest_data_point = latest_data_points.iloc[-1]

    # Create new data point for prediction
    new_data = {
        'latitude_lag': latest_data_point['latitude'],
        'longitude_lag': latest_data_point['longitude'],
        'sog_lag': latest_data_point['sog'],
        'cog_lag': latest_data_point['cog'] / 180 - 1,
        'rot_lag': latest_data_point['rot'],
        'heading_lag': latest_data_point['heading'] / 180 - 1,
        'time_diff_seconds': (pd.to_datetime(time) - latest_data_point['time']).total_seconds(),
        'sog_mean': latest_data_point['sog_mean'],
        'sog_std': latest_data_point['sog_std'],
        'hour': pd.to_datetime(time).hour,
        'day_of_week': pd.to_datetime(time).dayofweek,
        'is_weekend': 1 if pd.to_datetime(time).dayofweek >= 5 else 0
    }

    pred_lat = model_lat.predict([list(new_data.values())])[0]
    pred_lon = model_lon.predict([list(new_data.values())])[0]
    return pred_lat, pred_lon

# Making predictions on the test data and saving results
with open('aistest.csv', 'r') as f_test, open('predictions.csv', 'w') as f_pred:
    f_pred.write("ID,longitude_predicted,latitude_predicted\n")
    for line in tqdm(f_test.readlines()[1:]):
        id, vessel_id, time = line.strip().split(',')
        pred_lat, pred_lon = predict_future_position(vessel_id, time, train_data, best_rf_lat, rf)
        f_pred.write(f"{id},{pred_lon},{pred_lat}\n")
